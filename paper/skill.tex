\documentclass[a4paper,10pt]{article}

\usepackage{xltxtra} 
\usepackage{amsmath}
\usepackage[linkbordercolor={1 1 0}]{hyperref}
\usepackage[marginpar]{todo}

\usepackage[acronym,toc]{glossaries}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}

\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,mindmap}

\setromanfont[Mapping=tex-text]{Linux Libertine O}
% \setsansfont[Mapping=tex-text]{DejaVu Sans}
% \setmonofont[Mapping=tex-text]{DejaVu Sans Mono}

%funny makros we want to use
\newcommand{\den}[1]{[\![#1]\!]}
\newcommand{\skill}[0]{ SKilL }

%skill language definition
\lstdefinelanguage{skill}
{morekeywords={include,with,with,extends,annotation,const,auto,map,list,set,i1,i8,i16,i32,i64,v64,string,bool,f32,f64},
breakatwhitespace=true,
   breaklines=true,      
sensitive=false,
morecomment=[s]{/*}{*/},
morestring=[b]",
frameshape={nnn}{n}{y}{nyr},
}
\lstset{emph={%  
    tagged,class,indexed%
    },emphstyle={\color{red}\bfseries\underbar}%
}%

\title{The Serialization Killer Language}
\author{Timm Felden}
\date{\today}

\makeglossaries
\include{glossary}

\begin{document}
\maketitle

\begin{abstract}
 This work presents an alternative to various serialization approaches. The proposed serialization mechanism is fast, robust, extensible and easy to use. These goals are achieved by not using a human readable serialized form. \todo{blablabla}
\end{abstract}


\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
Main criticis: Erhard Plödereder and Martin Wittiger.

Additional criticis: Dominik Bruhn.
\end{abstract}

\todo{leider wird man nicht um einen glossar rumkommen. ABI, API, super type, base type, ...}
\todo{man muss klar definieren, was groundtypes und was basetypes sind und die begriffe dann auch konsistent benutzen}
\section{Motivation}

This paper presents an approach to serializing objects, which is tailored for usability, performance and portability. In order to achieve these goals, in contrast to XML, we will sacrifice generality and human readability of the serialized format. Unlike other general serialization mechanisms, we provide explicit support for extension points in the serialized data, in order to provide a minimum of upward compatibility and extensibility.

\subsection{Related Work}

There are many approaches similar to ours, but most of them have a different focus. This section shall provide a concise list of related approaches. For potential users of skill, this might also present alternatives superior for individual use cases.

The very nature of the problem requires different solutions, blablabla\todo{blabla}, skill is basically related to serialization like XML on one side and language interfaces like IDL or even JNI on the other side.

\subsection*{XML}

XML is a file format and might in fact be used as a backend. If a human readable storage on disk is not required, a binary encoding can be used to improve load/store performance significantly. \todo{proof!}


\subsubsection*{XML Schema definitions}

The description language itself is more or less equivalent to most schema definition languages such as XML Schema \todo{cite w3c}. The downside is that schema definitions have to operate on XML and can not directly be used with a binary format. There is also
no way to generate code for a client language, such as Ada, from schema definitions.

\subsubsection*{JAXP and xmlbeansxx}

For Java and C++, there are codegenerators, which can turn a XML schema file into code, which is able to deal with an xml in a similar way, as it is proposed by this work. In case of Java this is even in the standard library. The downside is, that, to our knowledge, this is only possible for Java and C++, thus it leaves us with portability issues. A minor problem of this approach is the lack of support for comment generation and the inefficient storage of serialized data.
An interesting observation is, that this approach deprives xml of its flexibility advantage over our solution. \todo{brr}


\subsection*{ASN.1}

Is not powerful enough to fit our purpose.

\subsection*{IDL}

\todo{ref David Lamb} Is not powerful enough and seems to be outdated.

It also uses an ASCII representation in order to store data, which does not fit our purpose.

Is itself superseded by XML.

On the other hand, it shares the basic architecture with this proposal.


\subsection*{Apatche Thrift \& Protobuf}

Lacks subtypeing. Protobuf has a overly complex notation language. Both seem to be optimized for network protocols, thus they do not have storage pools, which are the foundation of our serialization approach and an absolute requirement for some of our features, such as hints (see section \ref{hints}).


\subsection*{Language Specific}

Language specific is language specific and can therefore not be used to interface between subsystems written different programming languages such as Ada, Java, C or Haskell. Plus not every language offers such a mechanism. E.g. C.


\subsection*{Language Interfaces}
Language Interfaces do not permit serialization capabilities. Most language only provide interfaces for C, with varying quality and varying degree of automation. A significant problem are interfaces between languages with different memory models.
Interfaces between languages with different type systems are simply unproductive:D


\section{Syntax}

We use the tokens \verb/<id>/, \verb/<string>/, \verb/<int>/ and \verb/<comment>/. They equal C-style identifiers, strings, integer literals and comments respectively. Note that we use a comment token, which is need, because we want to emit the comments in the generated code, in order to integrate nicely into the target languages documentation system.

\begin{verbatim}
UNIT :=
  INCLUDE*
  DECLARATION*

INCLUDE := 
  ("include"|"with") <string> ";"?

DECLARATION :=
  DESCRIPTION
  <id>
  ((":"|"with"|"extends") <id>)?
  "{" FIELD* "}"
  
FIELD :=
  DESCRIPTION
  (CONSTANT|DATA) ";"?
  
DESCRIPTION := 
  (RESTRICTION|HINT)*
  <comment>?
  (RESTRICTION|HINT)*
  
RESTRICTION :=
  "@" <id> ("(" (R_ARG ("," R_ARG)*)? ")")? ";"?
  
R_ARG := ("%"|<int>|<string>)

HINT := "!" <id> ";"?
  
CONSTANT :=
  "const" TYPE <id> "=" <int>
  
DATA :=
  "auto"? TYPE <id>
  
TYPE :=
  ("map" MAPTYPE
  |"set" SETTYPE
  |"list" LISTTYPE
  |ARRAYTYPE)
  
MAPTYPE :=
  "<" GROUNDTYPE ("," GROUNDTYPE)+ ">"
  
SETTYPE :=
  "<" GROUNDTYPE ">"
  
LISTTYPE :=
  "<" GROUNDTYPE ">"
  
ARRAYTYPE :=
  GROUNDTYPE
  ("[" (<id>|<int>)? "]")?
  
GROUNDTYPE :=
  (<id>|"annotation")

\end{verbatim}
Note: The Grammar is LL(1).\footnote{In fact it can be expressed as a single regular expression.}

Comment: The optional \texttt{;} at the end of includes or definitions are for convenience only.

\subsection{Reserved Words}

The language itself has only the reserved words \textbf{annotation}, \textbf{auto}, \textbf{const}, \textbf{with}, \textbf{map}, \textbf{list} and \textbf{set}. \todo{check for updates}

However, it is strongly advised against using any identifiers which form reserved words in a potential target language, such as Ada, C++, C\#, Java, JavaScript or Python.
\todo{Appendix with a list of all identifiers which form reserved words in one of the languages above, including our keywords}

\subsection{Examples}

\begin{lstlisting}[label=blockExample,caption=Running Example,language=skill]
/** A source code location. */
SLoc {
  i16 line;
  i16 column;
  string path;
}

Block {
  SLoc begin;
  SLoc end;
  string image;
}

IfBlock : Block {
  Block thenBlock;
}

ITEBlock : IfBlock {
  Block elseBlock;
}
\end{lstlisting}

\subsubsection*{Includes, self references}

\begin{lstlisting}[label=example2a,caption=Example 2a,language=skill]
with "example2b.skill"

A {
  A a;
  B b;
}
\end{lstlisting}

\begin{lstlisting}[label=example2b,caption=Example 2b,language=skill]
with "example2a.skill"

B {
  A a;
}
\end{lstlisting}

\subsubsection*{Unicode}
The usage of non ASCII characters is completely legal, but discouraged.
\begin{lstlisting}[label=unicode,caption=Unicode Support,language=skill]
/** some arguably legal unicode characters. */
ö {
  ö ∀;
  ö €;
}
\end{lstlisting}




\section{Semantics}

This section will describe the meaning of individual keywords.

\subsection{Includes}
The file referenced by the with statement is processed as well. The declarations of all files reachable over \texttt{with} statements are collected, before any declaration is evaluated.

\subsection{\texttt{annotation}}
The type has a tag and a size, which allows it to be inserted at any annotation locations. This is useful in order to provide extension points in the file format. The file will still be readable by older implementations, which are not able to map any meaningful type into the annotation. A language binding is expected to provide something like an annotation proxy, which is used to represent annotation objects. If an application tries to get the object behind the proxy for an object of an unknown type, this will inevitably result in an error or exception. Therefore language bindings shall provide means of inspecting whether or not the type of the object behind an annotation is known.

As we will see in section \ref{serialization}, annotations are roughly equivalent to the type definition
\begin{verbatim}
annotation {
  v64 baseTypeName;
  v64 basePoolIndex;
}
\end{verbatim}
Of course, this is made transparent to the user and some language bindings will offer a special and type safe treatment of annotations.

\subsection{Sub Types}
A \gls{subType} of a \gls{userType} can be declared by appending the keyword \texttt{with} and the \gls{superType}s name to a declaration. In order to be well-formed, the sub type relation must remain acyclic and must not contain \glspl{unknownType}.

\subsection{\texttt{const}}
A const field can be used in order to create guards or version numbers, as well as overwriting deprecated fields with e.g. zeroes. The deserialization mechanism has to report an error if a constant field has an unexpected value.

\subsection{\texttt{auto}}
The language binding will create a field with the given type, but the content is transparent to the serialization mechanism. This is useful if the inference of the content of a field is likely to be faster then storing it, e.g. if it can be inferred lazy.

\subsection{Abstract Data Types}
\todo{rewrite section; it emerged from the fusion of two sections talking about ADTs}

\glspl{adt} showed to be useful and to increase the usability and understandability of the resulting code and file format.

\glspl{adt} are represented using arrays and pairs.

The type system has a built-in notion of arrays, maps, lists and sets. Note that all of them are, from the view of serialization, equivalent to length encodeded arrays. Their purpose is to increase the usability of the generated \gls{api}.


\subsection{Comments}
Comments provided in the skill file will be emitted into the generated code\footnote{If the target language does not allow for C-Style comments, the comments will be transformed in an appropriate way.}, thus allowing a user to profit from tooltips his IDE is likely to show him, containing this documentation. \todo{sprache!}



\section{The Type System}

The basic layout of the type system is:

\tikz [small mindmap, every node/.style=concept, concept color=black!20,
grow cyclic,
level 1/.append style={level distance=4.2cm,sibling angle=65},
level 2/.append style={level distance=2.7cm,sibling angle=40},
level 3/.append style={level distance=2cm,sibling angle=35},
level 4/.append style={level distance=1.5cm,sibling angle=35}
]
\node [root concept]{All Types}[clockwise from=0] % root
child { node {User Types}}
child { node{Compound Types}[clockwise from=30]
  child{ node{map}}
  child{ node{set}}
  child{ node{list}}
  child{ node{array}}
}
child { node{Built-In Types}[clockwise from=-20]
  child{ node{string}}
  child{ node{Float}[clockwise from=0]
    child{ node{f64}}
    child{ node{f32}}
  }
  child{ node{Integer}[clockwise from=-30]
    child{ node{v64}}
    child{ node{i64}}
    child{ node{i32}}
    child{ node{i16}}
    child{ node{i8}}
  }
  child{ node{bool}}
  child{ node{annotation}}
};

\Glspl{userType} can be seen as nonempty tuples over all types. \Glspl{builtInType} can be wrapped in order to give them special semantics. E.g. atime stamp can be created by:
\begin{lstlisting}[label=timeExample,caption=Time,language=skill]
time {
  /** seconds since 1.1.1970 0:00 UTC. */
  i64 date;
}
\end{lstlisting}

\subsection*{Common Abbreviations}

We will use some common abbreviations for sets of types in the rest of the manual:

Let \ldots
\begin{itemize}
 \item[\ldots] $\mathcal{T}$ be the set of all types
 \item[\ldots] $\mathcal{U}$ be the set of all user types
 \item[\ldots] $\mathcal{I}$ be the set of all integer types, i.e. $\{\texttt{i8},\texttt{i16},\texttt{i32},\texttt{i64},\texttt{v64}\}$
 \item[\ldots] $\mathcal{G}$ be the set of all ground types
\end{itemize}

We will call a type to be \textit{unknown}, if there is no declaration of the type, neither in the processed definition file, nor in any file transitively reachable through include directives. Such types must not occur in a declaration file, but they can be encountered in the serialization or deserialization process.


\subsection*{Legal Types}

% This section is to define the set of types, which can be used to declare legal fields inside a user type definition. Let $\mathcal{T}$ be the set of all types, $\mathcal{U} \subseteq \mathcal{T}$ be the set of user types, $\mathcal{G} \subseteq \mathcal{T}$ be the set of ground types, $\mathcal{C} \subseteq \mathcal{T}$ be the set of compound types and $\mathcal{I} \subseteq \mathcal{G}$ be the set of integer types.
% 
% In this context, we talk about an unknown, but fixed set $\mathcal{T}$, which corresponds to the contents of a set of input files. All types have unique names.

The given grammar of \skill already ensures that intuitive usage of the language will result in legal type declarations. The remaining aspects of illegal type declarations boil down to ill-formed usage of type and field names and can be summarized as:
\begin{itemize}
 \item Field names inside a type declaration must be unique inside the type and all its super types\footnote{The super type restriction may in fact be dropped?}.
 
 \item The subtype relation is a partial order\footnote{In fact it forms a forest.} and does not contain unknown types.

 \item For all fields f of dependent array type\footnote{E.g. a field \texttt{t[size] f} requires another field of integer type in the same declaration -- e.g. \texttt{i8 size}}, the size of the array has to denote a field of integer type in the very same declaration. The order of declaration is irrelevant.
 
 \item Any base type has to be known, i.e. it is either a ground type or it is a user type defined in any document transitively reachable over include commands.
\end{itemize}


\subsection*{Type Order}

Let $<_l$ be the lexical order. We define a partial order $\leq_t$ on $\mathcal{T}$ as follows:
\begin{itemize}
 \item $\forall t \in \mathcal{G}, s \in \mathcal{T}\setminus\mathcal{G}. t \leq_t s$
 \item $\forall t \in \mathcal{C}, s \in \mathcal{U}. t \leq_t s$
 \item $\forall s,t \in \mathcal{U}. t \leq_t s \leftarrow s <: t $\footnote{This is \textit{super types first}.}
 \item $\forall s,t \in \mathcal{U}. t \leq_t s = t \leq_l s\leftarrow \exists S \in \mathcal{U} \cup \{\bot\}. t <: S \wedge s <: S $\footnote{Types with the same or no supertype are order lexically.}
\end{itemize}

The informal short description is, first ground types, then compound types and user types at the end, where the forest of user types maintains its structure but is order using the lexical order of type names.

Notice, that this order corresponds to an left to right order in the types overview picture.

The missing order of compound types is left away intentionally, because it allows for the exchange of some type definition after publishing a format, e.g. \verb/t[] f/ can be exchanged with \verb/list<t> f/.


\subsection*{Strings}

Strings are conceptually a zero terminated sequence of utf8 encoded unicode characters. The in memory representation will try to make use of language features such as java.lang.String or std::u16string.

Strings must not contain 0 characters except for the terminating 0. If the users concept of a \textit{string} allows such data, he has to declare his own data type.

\subsection*{Compound Types}

The language offers several compound types. Sets, Lists and auto sized Arrays, i.e. arrays without an explicit size, are basically views onto the same kind of serialized data, i.e. they are a length encoded list of elements of the supplied base type. Arrays are expected to have a constant size, i.e. they are not guaranteed to be resizable. Sets are not allowed to contain the same element twice.
All ADTs will be mapped to their closest representation in the target language, while preserving these properties.
Maps are viewed as a representation of serializable partial functions. Therefore they can contain other map types as their second type argument, which is basically an instance of currying.

\subsection*{NULL Pointer}

The null pointer is serialized using the index 0. Conceptually, null pointers of different types are different. In fact if an annotation is a null pointer, it still has a type. However, this detail should not be observable in most languages.


\subsection{Examples}

This section will present some examples of ill-formed type declarations and brief explanations.

\begin{lstlisting}[label=stringExample,caption=Legal Super Types,language=skill]
EncodedString : string {
  string encoding;
}
\end{lstlisting}
Error: The built-in type ``string'' can not be sub classed.

\section{Restrictions}
Some invariants can be added to declarations and fields. These invariants can occur at the same place as comments, but can occur in any number. Invariants start with an \textsc{@} followed by a predicate. Each predicate has to supply a default argument \texttt{\%}, such that using only default arguments would not imply a restriction.
If multiple predicates are annotated, the conjunction of them forms the invariant.
The set of legal predicates is explained below.

If predicates, which are not directly applicable for compound types are used on compound types, they expand to the contents of the compound types, if applicable. Otherwise the usage of the predicate is illegal.

\subsection*{Range}
Range restrictions are used to restrict integers and floats.

Applies to fields: Integer, Float.

Signature: \verb/range(min, max)/: $\alpha \times \alpha → bool$

Defaults: obvious.

\begin{lstlisting}[label=rangeExample,caption=Examples,language=skill]
natural {
  @range(0,%)
  v64 data;
}
positive {
  @range(1,%)
  v64 data;
}
nonNegativeDouble {
  @range(0,%)
  f64 data;
}
\end{lstlisting}

\subsection*{NonNull}
Declares that an indexed field may not be null.

Applies to Field: Any indexed Type.

Signature: \verb/nonnull()/

Defaults: none.

\begin{lstlisting}[label=nonnullExample,caption=Examples,language=skill]
Node {
  @nonnull Node[] edges;
}
\end{lstlisting}


\subsection*{Unique}
Objects stored in a storage pool have to be distinct in their serialized form, i.e. for each pair of objects, there has to be at least one field, with a different value.

NOTE: This can cause difficulties in combination with sub-classing, because the uniqueness property must hold even on the part restricted to the topmost class declared to be unique.

Applies to Declarations of indexed types.

Signature: \verb/unique()/

Defaults: none.

\begin{lstlisting}[label=uniqueExample,caption=Examples,language=skill]
@unique Operator {
  string name;
}
@unique Term {
  Operator operator;
  Term[] arguments;
}
\end{lstlisting}


\subsection*{Singleton}
There is at most one instance of the declaration.

Applies to Declarations.

Signature: \verb/singleton()/

Defaults: none.

\begin{lstlisting}[label=singletonExample,caption=Examples,language=skill]
@singleton System { ... }
@singleton Data{
  /** Note: if data would not be a singleton itself, it is likely to violate the singleton property */
  System foo;
}
\end{lstlisting}


% \subsection*{Integer Renaming}
% In some languages it is possible to define subsets of integers by renameing the integer type. The type declaration must contain exactly one field of some integer type. The resulting type will be treated as integer type. Language support is encouraged to make use of language specific features such as typedefs. In Ada, this restriction should be processed together with the range restriction.


\subsection*{Ascription}
A language specific type can be ascribed to a field. The type has to be compatible to the fields actual type, because the ascription will not change the ABI in any way. The first argument is the language name. The second type is generator dependent, but should be related to types as they occur in local variable or field declaration in the respective language.

Although this kind of restriction puts a heavy burden on the language generator and decreases readability a lot, it can be used to increase the usability of the generated interface a lot, because language features such es enums in Java or unions and bitfields in C++ can be used.

Applies to fields.

Signature: \verb/as(language, type)/: $\texttt{string} \times \texttt{string} → \{\}$

Defaults: not allowed.

\begin{lstlisting}[label=ascriptionExample,caption=Examples,language=skill]
System {
  /**
  The language binding makes use of an enumeration, which is supplied with the generated code.
  
  The C++ interface will use the different type using C-Casts to convert between the two types (which is completely fine if the enum uses char as a base type).
  
  The Java interface will assume the stored integer to be the ordinal of the enum SystemState.
  */
  @as("C++", "ccast SystemState")
  @as("Java", "enum SystemState")
  i8 state
}
\end{lstlisting}



\subsection*{Tree}
The reference graph below created by objects of this type forms a tree. The type of the objects is irrelevant. Strings and fields with notree annotation, are not taken into account.

Applies to Declarations or Field.

Signature: \verb/tree()/

Defaults: none.


\subsubsection*{notree}
Applies to field.

Signature: \verb/notree()/

Defaults: none.

\begin{lstlisting}[label=treeExample,caption=Examples,language=skill]
Sloc{...}
@tree
SyntaktikEntity{
  /** not a tree, because several entities, might share them */
  @notree Sloc sloc;
  
  SyntaktikEntity[] children;
}
Routine {
  @notree
  Routine[] callers;
  @tree
  Routine[] dominators;
}

@tree
File {
  File[] children;
  /** several files could have the same name,
       but strings are implicitly @notree */
  string name;
  string content;
} 
\end{lstlisting}
Note: In case of the File example, there is no way to violate the tree property.
Note: It is legal for trees to form forests.


\section{Hints}
\label{hints}

Hints are annotations that start with a single \verb/!/ and are followed by a hint name.

\subsection*{Access}
Try to use a data structure that provides fast (random) access. E.g. an array list.

\subsection*{Modification}
Try to use a data structure that provides fast (random) modification. E.g. an linked list.

\subsection*{Unique}
Serialization shall unify objects with exactly the same serialized form. In combination with the @unique restriction, there shall at most be an error reported on deserialization.

\subsection*{Distributed}
Use a static map instead of fields to represent fields of definitions. This is usually an optimization if a definition has a lot of fields, but most use cases require only a small subset of them. Because hints do not modify the binary compatibility, some clients are likely to define the fields to be distributed or lazy.
\todo{f.a vs. a[f]}

\subsection*{Lazy}
Deserialize the fields data only if it is actually used. Lazy implies distributed.

\subsection*{Ignore}
The generated code is unable to access the respective field or any field of the type of the target declaration. This will lead to errors, if it is tried nonetheless. This option is provided to allow clients to reduce the memory footprint, if needed.



\section{On Extensibility and Canonical Field Order}

Extensibility is an important property. In this section, we develop a normal form of skill definitions, which will allow a robustness of a file format against modification. We will describe the effect of some changes, which can break decoding or encoding capabilities or break the API but not the file format (further ABI).

\subsection{Equality of Field Names}

Field names are equal, if their lexical representation is equal after converting all characters to lower case. Type declarations must not contain fields with equal names.

\subsection{Canonical Field Order}

A declaration is in canonical field order, if all fields are in type order and fields with the same or uncomparable type order are sorted in lexical order.

The type order relation is motivated by properties of compound types. The lexical order is motivated by the observation, that this order does not come with a cost, but provides some additional robustness against changes in definitions.


\subsection{Partial Types}

Objects can have partial types, if a part or the whole type of an object is unknown to the current binding\footnote{A problem that will arise frequently in the context of downward compatibility.}. It is important to understand that these objects can, under no circumstances, be deleted. If partial objects are encountered, one of the following actions should be taken (ordered by safety):
\begin{itemize}
 \item Serialization should be forbidden, i.e. the data is read-only.
 \item Only new objects may be added, thus the unknown objects can not be corrupted. This may however break an invariant of an unknown type.
 \item Only known objects will be serialized and all other objects will be discarded. This however is not an option for a lot of users.
\end{itemize}


\subsection{Sources of Incompatibility}

This section is to provide a concise list of changes and their effects on API and ABI compatibility.

\begin{itemize}
 \item A change of the organization of input files or the order of their definition has no effect.

 \item The addition of new declarations has no effect.

 \item A change regarding comments has no significant effect.

 \item A change in restrictions of any kind may break the API, potentially depending on the target programming language. It will most certainly change the set of legal files.

 \item A change of hints shall have no significant effect, although some applications can stop working after a change of hints, e.g. if they access fields which are annotated with \verb/!ignore/.

 \item Inserting or removing the keyword \texttt{const} may\todo{really?} break compatibility.

 \item Changing the value of a constant will break the ABI.\footnote{Which is btw. the very purpose of constants.}

 \item Inserting or removing the keyword \texttt{auto} will break ABI, but not the API.

 \item The presence of objects with unknown (sub)type with pointers to objects of known type may corrupt a file if it is written. This is even the case for annotations. It is therefore suggested, not to use pointers to existing objects, but to subtype them and to annotate objects in this fashion, because the data can be carried around correctly, even if the complete type of the object is not known. Another approach would be to disallow writing of such files.

 \item Any change of the structure of existing declarations, i.e. changing the modifier, adding or removing fields, etc., will break compatibility as a whole.
\end{itemize}




\section{Serialization}
\label{serialization}

This section is about representing objects as a sequence of bytes. We will call this sequence \textit{stream}, its formal Type will be named $S$, the current stream will be named $s$. We will assume that there is an implicit conversion between fixed sized integers\footnote{As well as between fixed sized floating point numbers, because we define them to be IEEE-754 encoded 32-/64-bit sequences.} and streams. We also make use of a stream concatenation operator $\circ : S \times S → S$.

This section assumes, that all objects about to be serialized are already known. It further assumes, that their types and thus the values of the functions (i.e. baseTypeName, typeName, index, $\den{\_}$) explained below can be easily computed.


\subsection{Steps of the Serialization Process}

In general it is assumed that the serialization process is split into the following steps:
\begin{enumerate}
 \item All objects to be serialized are collected. This is usually done using the transitive closure of an initial set.
 
 \item The items are organized into their storage pools, i.e. the index function is calculated.
 
 \item The output stream is created as described below.
\end{enumerate}

\subsection{Storage Pools}

This section contains the description of the high level file layout and gives meaning to the functions $baseTypeName: \mathcal{U} → S$, $typeName: \mathcal{U} → S$ and $index: \mathcal{U}\cup\{\textbf{string}\} → S$.

The basic idea behind the serialization format is to store the data grouped by type into storage pools. If objects are referred to from other objects, those references are usually given as integer, which is interpreted as index into the respective storage pool.

Because we will identify types by their name in human readable form, the first pool has to be the string pool. Because it is the first pool and the only pool, which stores objects of a built in type directly, it has a special layout. It starts with its size and is followed by exactly as many zero encoded utf8 strings. In skill inspired represenation, this would look like: \footnote{Note that the utf8 type is not predefined in skill; it is used to distinguish between the serialization of a string typed field and the very string object.}
\begin{verbatim}
v64 size;
utf8[size] stringPool;
\end{verbatim}

Now, as we have the first pool, we can explain the \textit{index} function. All objects are stored concise pools, which basically can be seen as arrays of these objects. The index function returns the index of the objects, starting with the index 1\footnote{This is a bit unfortunate, because it may cause some bugs in language binding generators, but using the v64 encoding, it is important to save the 0 for the NULL pointer, as it is encoded with 1 byte instead of 9 bytes as it is the case for -1, which would be the most obvious alternative.} for the first object. The NULL pointer is represented by the index 0.

Because we want to allow for full reflection capabilities, but we do not want to force users to pay for the mechanism, if they do not require it, we spend a byte to indicate whether the required data is present or not. The data is stored as the last object in the string pool. In skill this part would be:
\begin{verbatim}
bool lastStringIsDefinition;
\end{verbatim}

The remaining part is the serialization of the remaining nonempty storage pools. First, these pools are sorted in ascending type order. Then the fields defined of in the type stored in the pool are stored. Each pool keeps a start index, which allows for the reconstruction of the complete object. A short example shall illustrate the basic concept. It contains five types A,B,C,D and N. Each has a single field of type $\tau$ which is used to simplify the representation. The type Information for the objects in the base type pool can be inferred from the data stored in the pools using the links between the base type pool and the subtype pools (The \gls{baseType} start index (BPSI) field of pools with a super type -- shown as arrows). For the sake of readability, the name, size and count fields are omitted in the picture.

\begin{figure}[h]
 

%NOTE: if this picture is used in a paper, colors have to be replaced by stippled lines
\begin{tikzpicture}
%labels
\node[draw=none] at (-1,0.25) {$S$:};
\node[draw=none] at (-1,-0.25) {$\mathcal{T}$:};
\node[draw=none] at (-0.7,-1) {$index$:};
\node[draw=none] at (-0.85,-0.65) {\small{BPSI}:};
\node[draw=none] at (-0.9,2) {Def.:};

%definitions
\node[red,draw=none] at (2.5,2) {A \{ $\tau a$; \} };
\node[green,draw=none] at (1.5,1.5) {B : {\color{red} A } \{ $\tau b$; \} };
\node[blue,draw=none] at (1,1) {D : {\color{green} B } \{ $\tau d$; \} };
\node[black,draw=none] at (4,1.5) {C : {\color{red} A } \{ $\tau c$; \} };

\node[orange,draw=none] at (7.5,2) {N \{ $\tau n$; \} };

%content
 \node[color=black!60,draw=none] at (-0.35,0.25) {$\cdots$};
 
 \node[red,draw=none] at (0.25,0.25) {${a}$};
 \node[red,draw=none] at (0.75,0.25) {${a}$};
 \node[red,draw=none] at (1.25,0.25) {${a}$};
 \node[red,draw=none] at (1.75,0.25) {${a}$};
 \node[red,draw=none] at (2.25,0.25) {${a}$};
 \node[red,draw=none] at (2.75,0.25) {${a}$};
 
 \node[green,draw=none] at (3.45,0.25) {$b$};
 \node[green,draw=none] at (3.95,0.25) {${b}$};
 \node[green,draw=none] at (4.45,0.25) {${b}$};
 \node[green,draw=none] at (4.95,0.25) {${b}$};
 
 \node[blue,draw=none] at (5.65,0.25) {${d}$};
 
 \node[black,draw=none] at (6.35,0.25) {${c}$};
 
 \node[orange,draw=none] at (6.9,0.25) {${n}$};
 \node[orange,draw=none] at (7.55,0.25) {$\cdots$};
 
%base pool index
 
 \node[red,draw=none] at (0.25,-1) {1};
 \node[red,draw=none] at (0.75,-1) {2};
 \node[red,draw=none] at (1.25,-1) {3};
 \node[red,draw=none] at (1.75,-1) {4};
 \node[red,draw=none] at (2.25,-1) {5};
 \node[red,draw=none] at (2.75,-1) {6};
 
 \node[green,draw=none] at (3.45,-1) {2};
 \node[green,draw=none] at (3.95,-1) {3};
 \node[green,draw=none] at (4.45,-1) {4};
 \node[green,draw=none] at (4.95,-1) {5};
 
 \node[blue,draw=none] at (5.65,-1) {5};
 
 \node[black,draw=none] at (6.35,-1) {6};
 
 \node[orange,draw=none] at (6.9,-1) {1};
 \node[orange,draw=none] at (7.55,-1) {$\cdots$};
 
%base pool types
 \node[red,draw=none] at (0.25,-0.25) {A};
 \node[green,draw=none] at (0.75,-0.25) {B};
 \node[green,draw=none] at (1.25,-0.25) {B};
 \node[green,draw=none] at (1.75,-0.25) {B};
 \node[blue,draw=none] at (2.25,-0.25) {D};
 
 \node[black,draw=none] at (2.75,-0.25) {C};
 
 \node[orange,draw=none] at (6.9,-0.25) {N};

%frames
 \draw[shift={(0.15,0)},thick,orange] (6.4999,0) grid [step=0.5] (7.4,0.5);

 \draw[shift={(0.6,0)},thick,black] (5.3,0) grid [step=0.5] (6,0.5);
 \draw[shift={(0.4,0)},thick,blue] (4.8,0) grid [step=0.5] (5.5,0.5);
 \draw[shift={(0.2,0)},thick,green] (2.8,0) grid [step=0.5] (5,0.5);
 \draw[thick,red] (0,0) grid [step=0.5] (3,0.5);
 
%base type indices
 \draw[thick,green,->] (3.1,0.1) -- (3.1,-0.5) -- (0.5,-0.5) -- (0.5,0);
 \draw[thick,blue,->] (5.3,0.1) -- (5.3,-0.6) -- (2,-0.6) -- (2,0);
 \draw[blue,->] (5.3,0.1) -- (5.3,-0.6) -- (4.7,-0.6) -- (4.7,0);
 \draw[thick,black,->] (6,0.1) -- (6,-0.7) -- (2.5,-0.7) -- (2.5,0);
\end{tikzpicture}
\caption{The serialization scheme used to store objects into pools.}
\end{figure}

Note, that the pool of D comes in front of the pool of C, because it is smaller according to the type order.

This leads to the serialization of a pool as:
\begin{verbatim}
v64 poolName
v64 superIndex
option(v64 basePoolStartIndex; iff superIndex!=0)
v64 sizeCount
v64 sizeBytes
T[sizeCount] elements
\end{verbatim}

The poolName is an index into the string pool and points to the type name stored in the pool.

The superIndex is an index into the string pool and points to the name of the super type.

The sizeCount contains the number of elements in the pool. This is required in order to move objects of an unknown subtype. It does also simplify the deserialization process.

The basePoolStartIndex is the index of the first element in the base types pool.



A concise description of the file layout could look like this:
\begin{verbatim}
v64 size
utf8[size] stringPool

bool lastStringIsDefinition

while(next!=EOF){
  v64 poolName
  v64 superIndex
  option(v64 basePoolStartIndex; iff superIndex!=0)
  v64 sizeCount
  v64 sizeBytes
  [[ T[sizeCount] elements ]]
}
\end{verbatim}

An interesting observation about this encoding is, that the shortest legal file consists of two bytes, which are both zero. Although the encoding makes use of various invariants it provides some means of error detection, as explained in the appendix.

\subsection{Pool Elements}

Let $\den{\_}:\mathcal{T} → S$ be the translation function, which serializes a field of an object into a stream. The serialization of an objects takes places by serializing all its fields in canonical field order into the stream. In this section, we assume that the three functions defined in the last section are implicitly converted to streams using the v64 encoding. We assume further, that compound types provide a function $size: \mathcal{T} → \mathcal{I}$, which returns the number of elements stored in a given field.
Let $f$ be a field of type $t$, then $\den{f}$ is defined as\footnote{We will use C-Style hexadecimal integer literals for integers in streams.}
\begin{itemize}
 %null
 \item $\forall t \in \mathcal{U}\cup\{\textbf{string}\}. \den{\texttt{NULL}} = \texttt{0x00}$
 %pooled objects
 \item $\forall t \in \mathcal{U}\cup\{\textbf{string}\}. \den{f} = index(f)$
 
 %annotation -> * (v64 baseTypeName!!, v64index)
 \item $t=\textbf{annotation} \implies \den{\texttt{NULL}} = \texttt{0x00} \circ 0x00$\footnote{the first value is not important and may change in the future; it results from the encoding requiring a type for the null pointer.}
 \item $t=\textbf{annotation} \implies \den{f} = baseTypeName(f) \circ index(t)$
 
 %bool
 \item $t=\textbf{bool} \implies \den{\top} = \texttt{0xFF} \wedge \den{\bot} = \texttt{0x00}$
 
 %fixed int
 \item $\forall t \in \mathcal{I}\setminus\{\textbf{v64}\}. \den{f} = f$
 
 %v64
 \item $\forall t \in \{\textbf{v64}\}. \den{f} = encode(f)$\footnote{With encode as defined in listing \ref{v64enc}.}
 
 %fixed float
 \item $\forall t \in \{\textbf{f32},\textbf{f64}\}. \den{f} = f$\footnote{Assuming the float to be IEEE-754 encoded, which allows for an implicit bit-wise conversion to fixed sized integer.}
 
 %fixed and dependent arrays
 \item $\forall g \in \mathcal{G}, n \in \mathbb{N}^+. t = g\texttt{[}n\texttt{]} \implies \den{f} = \den{f_0} \circ \cdots \circ \den{f_{n-1}}$
 
 \item $\forall g \in \mathcal{G}, s \in \mathcal{I}, \texttt{s size}$\footnote{As stated above, size must be a field of the same declaration as f.} $. t = g\texttt{[size]} \wedge \texttt{size} \geq 0 \implies \den{f} = \den{f_0} \circ \cdots \circ \den{f_{\texttt{size}-1}}$\footnote{Note that this is the only case where the encoded field does not append anything to the stream.}
 
 %variable array, list and set
 \item $\forall g \in \mathcal{G}, n = size(f). t \in \{g\texttt{[]}, \texttt{set<}g\texttt{>}, \texttt{list<}g\texttt{>}\} \implies \den{f} = \den{n} \circ \den{f_0} \circ \cdots \circ \den{f_{n-1}}$
 
 %map
 \item Maps are serialized from left to right by serializing the keyset and ammending each key with the map structure which it points to. In case of Maps with two types, this is equal to a list of key value tuples.
 A field of type \verb/map<T,U,V>/ is serialized using a schema $ \den{size(f)} \circ \den{f.t_1} \circ \den{size(f[t_1])} \circ \den{f[t_1].u_1} \circ \den{f[t_1][u_1]} \circ \den{f[t_1].u_2} \circ \cdots \circ \den{size(f[t_2])} \circ \cdots \circ \den{f[t_n][u_m]}$
\end{itemize}




\section{Deserialization}

Deserialization is mostly straight forward.

The general strategy is:
\begin{itemize}
 \item the string pool is deserialized and a map from index to strings is created.
 \item the type structure is reconstructed while the pools are copied into memory
 \item objects are reconstructed
\end{itemize}


\section{API}

The generated \gls{api} has to be designed in a fashion, that integrates nicely with the languages programming paradigms. E.g. in Java it would be most useful to create a state object, which holds state of a bunch of serializable data and provides iterators over existing objcts, as well as factory methods and methods to remove objects form the state object. The serialized types can be represented by interfaces providing getters, setters, using hidden implementations, only known to the state object.

talk about the generated API and its features, like iterators, factories, access to singletons and stuff.

\subsection{Examples}

Nice example in C++:
\begin{lstlisting}[label=cppExample,caption=C++ Examples,language=C++]
#include <stdint.h>
#include <string>
[...some other bouilerplate includes...]
struct SLoc {
  uint16_t line;
  uint16_t column;
  std::string* path;
};
struct Block {
  std::string* tag;
  SLoc* begin;
  SLoc* end;
  std::string* image;
};
struct IfBlock : public Block {
  Block thenBlock;
};
struct ITEBlock : public IfBlock {
  Block elseBlock;
};
[...
  plus some boilerplate code for visitors, iostreams etc.
...]
\end{lstlisting}

\begin{lstlisting}[label=javaExample,caption=Java Examples,language=Java]
class SLoc {
  public short line;
  public short column;
  public String path;
}
class Block {
  final public String tag() {
    return this.getClass().getName();
  }
  public SLoc begin;
  public SLoc end;
  public String image:
}
class IfBlock extends Block {
  public Block thenBlock;
}
class ITEBlock extends IfBlock {
  public Block elseBlock;
}
[...some read and write code, plus some visitors...]
\end{lstlisting}


\begin{lstlisting}[label=latexExample,caption=LaTeX Examples,language={[LaTeX]TeX}]
$(line, column, path) \in SLoc
  \subseteq \mathbb{Z} \times \mathbb{Z} \times string$

$(begin, end, image) \in Block
  \subseteq SLoc \times SLoc \times string$

$(super, thenBlock) \in IfBlock
  \subseteq Block \times Block$

$(super, elseBlock) \in ITEBlock
  \subseteq IfBlock \times Block$
\end{lstlisting}
Which looks like:

$(line, column, path) \in SLoc \subseteq \mathbb{Z} \times \mathbb{Z} \times string$

$(begin, end, image) \in Block \subseteq SLoc \times SLoc \times string$

$(super, thenBlock) \in IfBlock \subseteq Block \times Block$

$(super, elseBlock) \in ITEBlock \subseteq IfBlock \times Block$

Note: The incentive of the \LaTeX-output is to provide a mechanism for users to formalize their file format using mechanisms, that are or can not be available as a specification language. E.g. the sentence ``The path of a SLoc points to a valid file on the file system and the line and column form a valid location inside that file.'' can not be verified in a static manner. This is because the correctness of the property depends not only on the content to be verified, but on the verifying environment as well.

\section{Case Study: Skill Encoded XML}
Although it is not very clever to use skill for encoding xml files, because one basically looses all benefits from both worlds, we will do so as demonstration for the compression yielded by the skill serialization scheme. Honestly most effects will be obtained from strings being stored in the string pool. Because most of the validation mechanisms directly built into xml are not required in skill and for the sake of simplicity, we will strip xml to its bare payload:
\begin{lstlisting}[label=sex,caption=Skill Encoded XML]
XML {
  string xmlDecl;
  Element element;
}
Element {
  string name;
  map<string, string> attributes;
  /** @note we will supply the empty string, if no
            content is present */
  string content;
  Element[] children;
}
\end{lstlisting}

\todo{compare size of some svg files}
\todo{compare speed of load/store of those svg files}
\todo{some final comments to say, that the comparison is of course not completely fair, and that it is advised against mixing xml and skill in most cases}

\section{Future Work}

XML output mit XML Schema.

Das neue Serialisierungsschema erlaubt es einen Viewer zu bauen, der Definition+Datei anzeigen kann. (Die future work ist hier der viewer)

Integration der Definition in die Serialisierte Form, damit man die Daten generisch prüfen und anzeigen kann. Hier braucht man noch ein gutes encoding, weil man sonst zu viel platz verbraucht.

Abuse annotations for type-safe unions. The type system does not allow for unrestricted unions or intersection types. The former violate serialization invariants, the latter would either have no instances or be equal to an already existing (super) type.

State somewhere, that a major advantage over XML is, that one is not required to link against a overly general implementation, which is nice if one is only interested in a very specific format.

Maybe it is possible to have an omit-string-pool-names option in some scenarios, but this would require all pools to be nonempty and would severely limit extensibility. An omit-pool-size-count option is more likely to come, because it would \textit{just} require all clients knowing all types.\footnote{Otherwise one would not be able to store a file containing unknown subtype data in general}

Fun fact: Garbage Collection for serializable objects comes for free, if objects are always held in storage pools.

\newpage
\todos

\part{Appendix}
\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}
\section{Variable Length Coding}

Size and Length information is stored as variable length coded 64 bit unsigned integers (aka C's \texttt{uint64\_t}). The basic idea is to use up to 9 bytes, where any byte starts with a 1 iff there is a consecutive byte. This leaves a payload of 7 bit for the first 8 bytes and 8 bits of payload for the ninth byte. This is very similar to the famous utf8 encoding and is motivated, as it is the case with utf8, by the assumption, that smaller numbers are a lot more likely. It has the nice property, that there are virtually no numerical size limitations.
The following small C++ functions will illustrate the algorithm:
\begin{lstlisting}[label=v64enc,caption=Variable Length Encoding,language=C++]
uint8_t* encode(uint64_t v){
  // calculate effective size
  int size = 0;
  {
    auto q = v;
    while(q){
      q >>= 7;
      size++;
    }
  }
  if(!size){
    auto rval = new uint8_t[1];
    rval[0]=0;
    return rval;
  }else if(10==size)
    size = 9;

  // split
  auto rval = new uint8_t[size];
  int count=0;
  for(;count<8&&count<size-1;count++){
    rval[count] = v >> (7*count);
    rval[count] |= 0x80;
  }
  rval[count] = v >> (7*count);
  return rval;
}
\end{lstlisting}
\begin{lstlisting}[label=v64dec,caption=Variable Length Decoding,language=C++]
uint64_t decode(uint8_t* p){
  int count = 0;
  uint64_t rval = 0;
  register uint64_t r;
  for(;count<8 && (*p)&0x80; count++, p++){
    r = p[0];
    rval |= (r&0x7f)<<(7*count);
  }
  r = p[0];
  rval |= (8==count?r:(r&0x7f))<<(7*count);
  return rval;
}
\end{lstlisting}


\section{Error Reporting}

This section describes some errors regarding ill-formatted files, which must be detected and reported. The order is based on the expected order of checking for the described error. The described errors are expected to be the result of file corruption, format change or bugs in a language binding.

\subsection*{Deserialization}
\begin{itemize}
\item If \texttt{EOF} is encountered unexpectedly, an error must be reported before producing any observable result.

\item If an index into a pool is invalid\footnote{because it is larger then the last string in the pool}, an error must be reported.

\item If the deserialization of a storage pool does not consume exactly the \texttt{sizeBytes} in its header, an error must be reported. Note: This is a strong indicator for a format change.

\item If the serialized type order of storage pools does not match the expected type order, an error must be reported.

\item If the serialized type information contains cycles, an error must be reported, which contains at least all type names in the detected cycle and the base type, if one can be determined.

\item If a storage pools contains elements which, based on their location in the base pool, should be subtypes of some kind, but have no respective sub type storage pool, an error must be reported with at least, the base type name, the most exact known type name and the adjacent base type names.

\item All known constant fields have to be checked before producing any observable result. If some constant value differs from the expected value, an error must be reported, which contains at least the type, the field type and name, the basePoolIndex, the index inside the types pool, the expected value and the actual value.

\item If a serialized value violates a restriction or the invariant of a type,\footnote{Including sets containing multiple similar elements.} an error must be reported as soon as this fact can be observed. It is explicitly not required to check all serialized data for this property.
\end{itemize}



\section{Reserved Words}
This section contains a table of words which must not be used as field names, because they are keywords in some languages. The usage of skill keywords will result in a direct error, whereas the usage of a word listed below will result in a warning, because the identifier will be escaped in the target language binding.


\textbf{
\begin{tabular}{ccccc}
if &then& else &begin &end\\
struct & class & public & protected &private \\
⇒ & ...
\end{tabular}
}


\section{Core Language}
The core language is a subset of the full language which must be supported by any generator, which is called skill core language generator. Features included in the core language are:
\begin{itemize}
 \item Integer types \texttt{i8} to \texttt{i64} and \texttt{v64}
 \item \texttt{string}, \texttt{bool} and \texttt{annotation}
 \item Compound types
 \item User Types with sub-typing
 \item \texttt{const} and \texttt{auto} fields.
\end{itemize}

Thus the remaining parts required for full skill support are:
\begin{itemize}
 \item Floats
 \item Restrictions
 \item Hints
 \item Language dependent treatment of comments, e.g. integration into doxygen or javadoc.\footnote{This may even require a language extension providing tags inside comments which are translated into tags of the respective documentation framework.}
 \item Reflection?
 \item Name mangeling to allow for usage of language keywords or illegal characters (unicode) in specification files, without making a language binding impossible.
\end{itemize}


\newpage
%%% TODO REMOVE NEXT LINE
\glsaddall
\printglossaries

\end{document}
