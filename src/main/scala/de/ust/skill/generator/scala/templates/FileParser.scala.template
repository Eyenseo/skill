
        case _ ⇒ new StoragePool(t, t.superName match {
          case Some(name) ⇒ σ.pools.get(name).ensuring(_.isDefined)
          case None       ⇒ None
        }, blockCounter)
      }
      σ.pools.put(t.name, result)
      t.subTypes.foreach(makePool(_))
      result
    }

    // make base pools; the makePool function makes sub pools
    userTypeIndexMap.values.filter(_.superName.isEmpty).foreach({ b ⇒
      val p = makePool(b)
      if (p.isInstanceOf[KnownPool])
        p.asInstanceOf[KnownPool].constructPool
      else
        println(s"unknown pool $p")
    })
  }

  /**
   * A type declaration, as it occurs during parsing of a type blocks header.
   *
   * @author Timm Felden
   */
  private class TypeDeclaration(
      val name: String,
      val superName: Option[String],
      val lbpsi: Long,
      val count: Long /*, restrictions*/ ,
      val fieldCount: Long,
      // null iff the type occurred first in this block
      var userType: UserType) {

    // ensure presence of lbpsi type start indices
    if (superName.isEmpty && !localBlockBaseTypeStartIndices.contains(name))
      localBlockBaseTypeStartIndices.put(name, 0L)

    /**
     * Field declarations obtained from the header.
     */
    var fieldDeclarations: List[FieldDeclaration] = null

    /**
     * Reads field declarations matching this type declaration from a stream, based on the state σ
     *
     * TODO treat restrictions
     */
    def parseFieldDeclarations: Parser[TypeDeclaration] = tryCatch({
      // if we have new instances and the type existed and there are new fields, we get into special cases
      if (null != userType && count > 0) {
        val knownFields = userType.fields.size
        assert(knownFields > 0, s"The block contains lacks $knownFields field declarations.")
        var fieldIndex = 0

        tryCatch(repN(knownFields.toInt,
          v64 ^^ { end ⇒
            var result = userType.fields(fieldIndex)
            result.end = end

            result
          }
        ) >> { fields ⇒
            repN((fieldCount - knownFields).toInt, restrictions ~ fieldTypeDeclaration ~ v64 ~ v64 ^^ {
              case r ~ t ~ n ~ end ⇒
                val result = new FieldDeclaration(t, σ(n), end)
                userType.fields.append(result)
                result
            }) ^^ { newFields ⇒
              fields ++ newFields
            }
          }
        )(s"partial type declaration in type $name failed")
      } else {
        // we append only fields to the type; it is not important whether or not it existed before;
        //  all fields contain all decalrations
        tryCatch(
          repN(fieldCount.toInt,
            restrictions ~ fieldTypeDeclaration ~ v64 ~ v64 ^^ {
              case r ~ t ~ n ~ end ⇒
                val name = σ(n)
                new FieldDeclaration(t, name, end)
            })
        )(s"full type declaration in type $name failed")
      }
    } ^^ { r ⇒
      fieldDeclarations = r;
      this
    })(s"failed to parse a field of $name")

  }

  /**
   * the state to be created is shared across a file parser instance; file parser instances are used to turn a file into
   * a new state anyway.
   */
  private val σ = new SerializableState;

  // helper structures required to build user types
  private var userTypeIndexMap = new HashMap[Long, UserType]
  private var userTypeNameMap = new HashMap[String, UserType]
  private var blockCounter = 0;

  /**
   * The map contains start indices of base types, which are required at the end of the type header processing phase to
   *  create absolute base pool indices from relative ones.
   */
  private var localBlockBaseTypeStartIndices = new HashMap[String, Long]

  /**
   * @param σ the processed serializable state
   * @return a function that maps logical indices to the corresponding strings
   */
  private[this] implicit def poolAccess(σ: SerializableState): (Long ⇒ String) = σ.getString(_)

  /**
   * turns a file into a raw serializable state
   */
  private def file: Parser[SerializableState] = {
    tryCatch(
      rep(hasMore ~>
        stringBlock ~
        (typeBlock ^^ { declarations ⇒
          // update counters
          declarations.filter(_.superName.isEmpty).foreach({ d ⇒
            localBlockBaseTypeStartIndices.put(d.name, d.userType.instanceCount)
          })

          blockCounter += 1
          ()
        })
      ) ^^ { _ ⇒ makeState; σ }
    )(s"file parsing failed in block $blockCounter ")
  }

  /**
   * reads a string block
   */
  private def stringBlock: Parser[Unit] = tryCatch(
    tryCatch(
      v64 >> { count ⇒ repN(count.toInt, i32) }
    )("string block head corrupted")
      >> stringBlockData
  )("string block parsing failed")

  private def stringBlockData(offsets: List[Int]) = tryCatch(new Parser[Unit] {
    def apply(in: Input) = {
      // add absolute offsets and lengths to the states position buffer
      var it = offsets.iterator
      var last = 0
      val off = in.position
      val map = σ.strings.stringPositions

      while (it.hasNext) {
        val next = it.next()
        map.put(map.size + 1, (off + last, next - last))
        last = next
      }

      // jump over string data
      Success((), in.drop(last.toInt))
    }
  })("string offset processing failed")

  /**
   * reads a type block and adds the contained type information to the pool.
   * At the moment, it will process fields as well, because we do not have a good random access file stream
   *  implementation, yet.
   */
  private def typeBlock: Parser[Array[TypeDeclaration]] = tryCatch(
    (v64 >> { count ⇒ repN(count.toInt, typeDeclaration) } ^^ { rawList: List[TypeDeclaration] ⇒
      val raw = rawList.toArray

      // create new user types for types that did not exist until now
      raw.filter(null == _.userType).foreach({ td ⇒
        val u = new UserType(
          userTypeIndexMap.size,
          td.name,
          td.superName,
          new ArrayBuffer[FieldDeclaration] ++= td.fieldDeclarations
        )
        userTypeIndexMap.put(u.index, u)
        userTypeNameMap.put(u.name, u)
        td.userType = u
      })

      // set super and base types of new user types
      // note: this requires another pass, because super types may be defined after a type
      def mkBase(t: UserType): Unit = if (null == t.baseType) {
        val s = userTypeNameMap(t.superName.get)
        mkBase(s)
        t.superType = s
        t.baseType = s.baseType
        s.subTypes += t
      }
      // note rather inefficient @see issue #9
      raw.foreach({ d ⇒ mkBase(d.userType) })

      raw.foreach({ t ⇒
        val u = t.userType
        // add local block info
        u.addBlockInfo(new BlockInfo(t.count, localBlockBaseTypeStartIndices(u.baseType.name) + t.lbpsi), blockCounter)

        // eliminate preliminary user types in field declarations
        u.fields.foreach(f ⇒ {
          if (f.t.isInstanceOf[PreliminaryUserType]) {
            val index = f.t.asInstanceOf[PreliminaryUserType].index
            if (userTypeIndexMap.contains(index))
              f.t = userTypeIndexMap(index)
            else
              throw new SkillException(
                s"${t.name}.${f.name} refers to an invalid user type $index (user types: ${
                  userTypeIndexMap.mkString(", ")
                })"
              )
          }
        })
      })

      raw
    }) >> typeChunks
  )("type block parsing failed")

  /**
   * reads type chunks using the raw information from the type block
   */
  private def typeChunks(declarations: Array[TypeDeclaration]) = new Parser[Array[TypeDeclaration]] {
    def apply(in: Input) = try {

      var lastOffset = 0L;

      declarations.foreach({ d ⇒
        d.fieldDeclarations.foreach({ f ⇒
          // the stream is at $lastOffset; we want to read until the fields offset
          // TODO even if the field had not yet existed but instances had?
          f.dataChunks ++= List(ChunkInfo(in.position + lastOffset, f.end - lastOffset, d.count))
          lastOffset = f.end

          //invalidate end for security reasons
          f.end = -1
        })
      })

      // jump over the data chunk, it might be processed in the future
      in.drop(lastOffset.toInt)

      Success(declarations, in)
    } catch { case e: SkillException ⇒ SkillException("type chunk parsing failed", e) }
  }

  /**
   * see skill ref man §6.2
   */
  private[this] def typeDeclaration: Parser[TypeDeclaration] = tryCatch(
    (v64 ^^ { i ⇒ σ(i) }) >> { name ⇒
      tryCatch(
        // check if we append to an existing type
        if (userTypeNameMap.contains(name)) {
          val t = userTypeNameMap(name)
          var s: Option[String] = None

          tryCatch(
            superInfo(t.superName) ~ v64 ~ v64 ^^ {
              case lbpsi ~ count ~ fields ⇒
                new TypeDeclaration(name, s, lbpsi, count, fields, t)
            })(s"failed to parse type $name")

        } else {
          tryCatch(
            (v64 >> superInfo) ~ v64 ~ restrictions ~ v64 ^^ {
              case (sup, lbpsi) ~ count ~ restrictions ~ fields ⇒
                new TypeDeclaration(name, sup, lbpsi, count, fields, null)
            })(s"failed to parse new type $name")
        }
      )(s"type: $name")
    } >> { _.parseFieldDeclarations }
  )("type declaration parsing failed")

  /**
   *  @return a tuple with (super name, super index)
   */
  private[this] def superInfo(index: Long) = {
    if (index != 0)
      v64 ^^ { lbpsi ⇒ (Some(σ(index)), lbpsi) }
    else
      success((None, 1L))
  }

  /**
   * return a parser parsing local base pool start index, if necessary
   */
  private[this] def superInfo(superName: Option[String]) = superName match {
    case Some(_) ⇒ v64
    case None    ⇒ success(1L)
  }

  /**
   * restrictions are currently restored to their textual representation
   */
  private[this] def restrictions: Parser[List[String]] = v64 >> { i ⇒ repN(i.toInt, restriction) }
  private[this] def restriction = tryCatch(
    v64 >> { i ⇒
      i match {
        case 0 ⇒ try { v64 ~ v64 ~ v64 ^^ { case l ~ r ~ b ⇒ s"range(${σ(l)}, ${σ(r)}, ${σ(b)})" } }
        catch { case e: Exception ⇒ SkillException("malformed range extension", e) }

        case 1  ⇒ success("nullable")
        case 2  ⇒ success("unique")
        case 3  ⇒ success("singleton")
        case 4  ⇒ success("constantLengthPointer")
        case 5  ⇒ success("monotone")
        case id ⇒ throw new java.lang.Error(s"Restrictions ID $id not yet supported!")
      }
    }
  )("restriction parsing failed")

  /**
   * Turns a field type into a preliminary type information. In case of user types, the declaration of the respective
   *  user type may follow after the field declaration.
   */
  private def fieldTypeDeclaration: Parser[TypeInfo] = try {
    v64 >> { i ⇒
      i match {
        case 0            ⇒ i8 ^^ { new ConstantI8Info(_) }
        case 1            ⇒ i16 ^^ { new ConstantI16Info(_) }
        case 2            ⇒ i32 ^^ { new ConstantI32Info(_) }
        case 3            ⇒ i64 ^^ { new ConstantI64Info(_) }
        case 4            ⇒ v64 ^^ { new ConstantV64Info(_) }
        case 5            ⇒ success(new AnnotationInfo())
        case 6            ⇒ success(new BoolInfo())
        case 7            ⇒ success(new I8Info())
        case 8            ⇒ success(new I16Info())
        case 9            ⇒ success(new I32Info())
        case 10           ⇒ success(new I64Info())
        case 11           ⇒ success(new V64Info())
        case 12           ⇒ success(new F32Info())
        case 13           ⇒ success(new F64Info())
        case 14           ⇒ success(new StringInfo())
        case 15           ⇒ v64 ~ baseTypeInfo ^^ { case i ~ t ⇒ new ConstantLengthArrayInfo(i.toInt, t) }
        case 17           ⇒ baseTypeInfo ^^ { new VariableLengthArrayInfo(_) }
        case 18           ⇒ baseTypeInfo ^^ { new ListInfo(_) }
        case 19           ⇒ baseTypeInfo ^^ { new SetInfo(_) }
        case 20           ⇒ v64 >> { n ⇒ repN(n.toInt, baseTypeInfo) } ^^ { new MapInfo(_) }
        case i if i >= 32 ⇒ success(new PreliminaryUserType(i - 32))
        case id           ⇒ ParseException(s"invalid type ID: $id")
      }
    }
  } catch { case e: SkillException ⇒ SkillException("field type parsing failed", e) }

  /**
   * matches only types which are legal arguments to ADTs
   */
  private def baseTypeInfo: Parser[TypeInfo] = v64 ^^ { i ⇒
    i match {
      case 5            ⇒ new AnnotationInfo()
      case 6            ⇒ new BoolInfo()
      case 7            ⇒ new I8Info()
      case 8            ⇒ new I16Info()
      case 9            ⇒ new I32Info()
      case 10           ⇒ new I64Info()
      case 11           ⇒ new V64Info()
      case 12           ⇒ new F32Info()
      case 13           ⇒ new F64Info()
      case 14           ⇒ new StringInfo()
      case i if i >= 32 ⇒ new PreliminaryUserType(i - 32)
    }
  }

  def readFile(path: Path): SerializableState = {
    try {
      σ.fromReader = new ByteReader(Files.newByteChannel(path).asInstanceOf[FileChannel])
      val in = σ.fromReader
      file(in) match {
        case Success(r, i) ⇒ r
        case NoSuccess(msg, i) ⇒ throw new SkillException(
          s"""Failed to parse ${path}:
  Message: $msg
  Got stuck at byte ${in.pos.column} with at least ${in.minimumBytesToGo} bytes to go.
  The next Byte is a ${try { in.next.toHexString } catch { case _: Exception ⇒ "EOF" }}.
        """)
      }
    } catch {
      case e: SkillException ⇒ throw new SkillException(s"failed to read file $path", e)
    }
  }
}

object FileParser extends ByteStreamParsers {
  import FileParser._

  /**
   * reads the contents of a file, creating a new state
   */
  def read(path: Path): SerializableState = (new FileParser).readFile(path)
}
