
  /**
   * A type decalariation, as it occurs during parsing of a type blocks header.
   *
   * @author Timm Felden
   */
  private class TypeDeclaration(
      val name: String,
      val superName: Option[String],
      val lbpsi: Long,
      val count: Long /*, restrictions*/ ,
      val fieldCount: Long) {
    var fieldDeclarations: List[FieldDeclaration] = null

    /**
     * Reads field declarations matching this type declaration from a stream, based on the state σ
     *
     * TODO treat restrictions
     */
    def parseFieldDeclarations: Parser[TypeDeclaration] = {
      // if we have new instances and the type existed and there are new fields, we get into special cases
      if (count > 0 && σ.pools.contains(name)) {
        val knownFields = σ.pools(name).userType.fields.size
        assert(knownFields > 0, s"The block contains lacks $knownFields field declarations.")
        var fieldIndex = 0

        tryCatch(repN(knownFields.toInt, v64 ^^ { end ⇒
          var result = σ.pools(name).userType.fields(fieldIndex)
          result.end = end

          result
        }) >> { fields ⇒
          repN((fieldCount - knownFields).toInt, restrictions ~ fieldTypeDeclaration ~ v64 ~ v64 ^^ {
            case r ~ t ~ n ~ end ⇒
              new FieldDeclaration(t, σ(n), end)
          }) ^^ { newFields ⇒
            fields ++ newFields
          }
        }
        )(s"partial type declaration in type $name failed")
      } else {
        // we append only fields to the type; it is not important whether or not it existed before;
        //  all fields contain all decalrations
        tryCatch(
          repN(fieldCount.toInt,
            restrictions ~ fieldTypeDeclaration ~ v64 ~ v64 ^^ {
              case r ~ t ~ n ~ end ⇒
                val name = σ(n)
                new FieldDeclaration(t, name, end)
            })
        )(s"full type declaration in type $name failed")
      }
    } ^^ { r ⇒
      fieldDeclarations = r;
      this
    }

  }

  /**
   * the state to be created is shared across a file parser instance; file parser instances are used to turn a file into
   * a new state anyway.
   */
  private val σ = new SerializableState;

  private var userTypeIndexMap = new HashMap[Long, UserType]

  /**
   * @param σ the processed serializable state
   * @return a function that maps logical indices to the corresponding strings
   */
  private[this] implicit def poolAccess(σ: SerializableState): (Long ⇒ String) = σ.getString(_)

  /**
   * turns a file into a raw serializable state
   */
  private def file: Parser[SerializableState] = {
    var blockCounter = 1;
    tryCatch(
      rep(hasMore ~>
        stringBlock ~
        typeBlock ~ { blockCounter += 1; success() }
      ) ^^ { _ ⇒ σ }
    )(s"file parsing failed in block $blockCounter ")
  }

  /**
   * reads a string block
   */
  private def stringBlock: Parser[Unit] = tryCatch(
    tryCatch(
      v64 >> { count ⇒ repN(count.toInt, i32) }
    )("string block head corrupted")
      >> stringBlockData
  )("string block parsing failed")

  private def stringBlockData(offsets: List[Int]) = tryCatch(new Parser[Unit] {
    def apply(in: Input) = {
      // add absolute offsets and lengths to the states position buffer
      var it = offsets.iterator
      var last = 0
      val off = in.position
      val map = σ.strings.stringPositions

      while (it.hasNext) {
        val next = it.next()
        map.put(map.size + 1, (off + last, next - last))
        last = next
      }

      // jump over string data
      Success((), in.drop(last.toInt))
    }
  })("string offset processing failed")

  /**
   * reads a type block and adds the contained type information to the pool.
   * At the moment, it will process fields as well, because we do not have a good random access file stream
   *  implementation, yet.
   */
  private def typeBlock: Parser[Unit] = tryCatch(
    (v64 >>
      { count ⇒ repN(count.toInt, typeDeclaration) } ^^ { rawList: List[TypeDeclaration] ⇒
        val raw = rawList.toArray

        var i = userTypeIndexMap.size
        var refl = raw.map({
          decl ⇒
            assert(null != decl.fieldDeclarations, s"forgot to parse field declarations of type ${decl.name}")

            val rval = new UserType(i, decl.name, decl.superName,
              new ArrayBuffer[FieldDeclaration] ++= decl.fieldDeclarations
            );
            userTypeIndexMap.put(rval.index, rval)
            i += 1;
            rval
        }).toArray

        // eliminate preliminary user types
        refl.foreach({ t ⇒
          t.fields.foreach(f ⇒ {
            if (f.t.isInstanceOf[PreliminaryUserType]) {
              val index = f.t.asInstanceOf[PreliminaryUserType].index
              if (userTypeIndexMap.contains(index))
                f.t = userTypeIndexMap(index)
              else
                throw new SkillException(
                  s"${t.name}.${f.name} refers to an invalid user type $index (last valid is ${userTypeIndexMap.size})"
                )
            }
          })
        })

        (raw, refl)
      }) >> { arg ⇒
        typeChunks(arg._1) ^^ {
          case () ⇒
            val raw = arg._1;
            // create storage pools
            var i = 0
            arg._2.foreach({ t ⇒
              σ.pools.put(t.name, makePool(σ, t.name, t, t.superName match {
                case Some(s: String) ⇒ σ.pools.get(s)
                case None            ⇒ None
              }, raw(i).count, raw(i).lbpsi));
              i += 1
            })
        }
      }
  )("type block parsing failed")

  /**
   * reads type chunks using the raw information from the type block
   */
  private def typeChunks(declarations: Array[TypeDeclaration]) = new Parser[Unit] {
    def apply(in: Input) = try {

      var lastOffset = 0L;

      declarations.foreach({ d ⇒
        d.fieldDeclarations.foreach({ f ⇒
          // the stream is at $lastOffset; we want to read until the fields offset
          f.dataChunks ++= List(ChunkInfo(in.position, f.end - lastOffset, d.count))
          lastOffset = f.end

          //invalidate end for security reasons
          f.end = -1
        })
      })

      // jump over the data chunk, it might be processed in the future
      in.drop(lastOffset.toInt)

      Success((), in)
    } catch { case e: SkillException ⇒ SkillException("type chunk parsing failed", e) }
  }

  /**
   * see skill ref man §6.2
   */
  private[this] def typeDeclaration: Parser[TypeDeclaration] = tryCatch(
    (v64 ^^ { i ⇒ σ(i) }) >> { name ⇒
      // check if we append to an existing type
      if (σ.pools.contains(name)) {
        val t = σ.pools(name)
        var s: Option[String] = None

        tryCatch((t.superPool match {
          case Some(sup) ⇒
            s = Some(sup.name); v64
          case _ ⇒ success(0L)
        }) ~ v64 ~ v64 ^^ {
          case lbpsi ~ count ~ fields ⇒
            new TypeDeclaration(name, s, lbpsi, count, fields)
        })(s"failed to parse type $name")

      } else {
        tryCatch(((v64 >> superInformation) ~
          v64 ~
          restrictions ~
          v64
        ) ^^ {
            case sup ~ count ~ restrictions ~ fields ⇒
              new TypeDeclaration(name, sup._1, sup._2, count, fields)
          })(s"failed to parse new type $name")
      }
    } >> { _.parseFieldDeclarations }
  )("type declaration parsing failed")

  /**
   *  @return a tuple with (super name, super index)
   */
  private[this] def superInformation(index: Long) = {
    if (index != 0)
      v64 ^^ { i ⇒ (Some(σ(i)), index) }
    else
      success((None, 0L))
  }

  /**
   * restrictions are currently restored to their textual representation
   */
  private[this] def restrictions: Parser[List[String]] = v64 >> { i ⇒ repN(i.toInt, restriction) }
  private[this] def restriction = tryCatch(
    v64 >> { i ⇒
      i match {
        case 0 ⇒ try { v64 ~ v64 ~ v64 ^^ { case l ~ r ~ b ⇒ s"range(${σ(l)}, ${σ(r)}, ${σ(b)})" } }
        catch { case e: Exception ⇒ SkillException("malformed range extension", e) }

        case 1  ⇒ success("nullable")
        case 2  ⇒ success("unique")
        case 3  ⇒ success("singleton")
        case 4  ⇒ success("constantLengthPointer")
        case 5  ⇒ success("monotone")
        case id ⇒ throw new java.lang.Error(s"Restrictions ID $id not yet supported!")
      }
    }
  )("restriction parsing failed")

  /**
   * Turns a field type into a preliminary type information. In case of user types, the declaration of the respective
   *  user type may follow after the field declaration.
   */
  private def fieldTypeDeclaration: Parser[TypeInfo] = try {
    v64 >> { i ⇒
      i match {
        case 0            ⇒ i8 ^^ { new ConstantI8Info(_) }
        case 1            ⇒ i16 ^^ { new ConstantI16Info(_) }
        case 2            ⇒ i32 ^^ { new ConstantI32Info(_) }
        case 3            ⇒ i64 ^^ { new ConstantI64Info(_) }
        case 4            ⇒ v64 ^^ { new ConstantV64Info(_) }
        case 5            ⇒ success(new AnnotationInfo())
        case 6            ⇒ success(new BoolInfo())
        case 7            ⇒ success(new I8Info())
        case 8            ⇒ success(new I16Info())
        case 9            ⇒ success(new I32Info())
        case 10           ⇒ success(new I64Info())
        case 11           ⇒ success(new V64Info())
        case 12           ⇒ success(new F32Info())
        case 13           ⇒ success(new F64Info())
        case 14           ⇒ success(new StringInfo())
        case 15           ⇒ v64 ~ baseTypeInfo ^^ { case i ~ t ⇒ new ConstantLengthArrayInfo(i.toInt, t) }
        case 17           ⇒ baseTypeInfo ^^ { new VariableLengthArrayInfo(_) }
        case 18           ⇒ baseTypeInfo ^^ { new ListInfo(_) }
        case 19           ⇒ baseTypeInfo ^^ { new SetInfo(_) }
        case 20           ⇒ v64 >> { n ⇒ repN(n.toInt, baseTypeInfo) } ^^ { new MapInfo(_) }
        case i if i >= 32 ⇒ success(new PreliminaryUserType(i - 32))
        case id           ⇒ ParseException(s"invalid type ID: $id")
      }
    }
  } catch { case e: SkillException ⇒ SkillException("field type parsing failed", e) }

  /**
   * matches only types which are legal arguments to ADTs
   */
  private def baseTypeInfo: Parser[TypeInfo] = v64 ^^ { i ⇒
    i match {
      case 5            ⇒ new AnnotationInfo()
      case 6            ⇒ new BoolInfo()
      case 7            ⇒ new I8Info()
      case 8            ⇒ new I16Info()
      case 9            ⇒ new I32Info()
      case 10           ⇒ new I64Info()
      case 11           ⇒ new V64Info()
      case 12           ⇒ new F32Info()
      case 13           ⇒ new F64Info()
      case 14           ⇒ new StringInfo()
      case i if i >= 32 ⇒ new PreliminaryUserType(i - 32)
    }
  }

  def readFile(path: Path): SerializableState = {
    try {
      σ.fromReader = new ByteReader(Files.newByteChannel(path).asInstanceOf[FileChannel])
      val in = σ.fromReader
      file(in) match {
        case Success(r, i) ⇒ r
        case NoSuccess(msg, i) ⇒ throw new SkillException(
          s"""Failed to parse ${path}:
  Message: $msg
  Got stuck at byte ${in.pos.column} with at least ${in.minimumBytesToGo} bytes to go.
  The next Byte is a ${try { in.next.toHexString } catch { case _: Exception ⇒ "EOF" }}.
        """)
      }
    } catch {
      case e: SkillException ⇒ throw new SkillException(s"failed to read file $path", e)
    }
  }
}

object FileParser extends ByteStreamParsers {
  import FileParser._

  /**
   * reads the contents of a file, creating a new state
   */
  def read(path: Path): SerializableState = (new FileParser).readFile(path)
}